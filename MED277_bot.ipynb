{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function loads discharge summary data from MIMIC-III dataset NOTEEVENTS.csv.gz file.\n",
    "update the base_path to be the location of this zipped '''\n",
    "def load_data():\n",
    "    ## Intitializing data paths\n",
    "    base_path = r'D:\\ORGANIZATION\\UCSD_Life\\Work\\4. Quarter-3\\Subjects\\MED 277\\Project\\DATA\\\\'\n",
    "    data_file = base_path+\"NOTEEVENTS.csv.gz\"\n",
    "    \n",
    "    ## Loading data frames from CSV file\n",
    "    df = pd.read_csv(data_file, compression='gzip')\n",
    "    \n",
    "    ## Uncomment this to slice the size of dataset\n",
    "    #df = df[:10000]\n",
    "    \n",
    "    ## Uncomment this to save processed data to the memory\n",
    "    #joblib.dump(df,base_path+'data10.pkl')\n",
    "    ## loading data frames from PKL memory\n",
    "    #df1 =  joblib.load(base_path+'data10.pkl')\n",
    "    \n",
    "    ## Filtering dataframe for \"Discharge summaries\" and \"TEXT\"\n",
    "    df = df.loc[df['CATEGORY'] == 'Discharge summary'] #Extracting only discharge summaries\n",
    "    df_text = df['TEXT']\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT ALL THE TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that processes the entire document string'''\n",
    "def process_text(txt):\n",
    "    txt1 = re.sub('[\\n]',\" \",txt)\n",
    "    txt1 = re.sub('[^A-Za-z \\.]+', '', txt1)\n",
    "    \n",
    "    return txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that processes the document string not considering separate lines'''\n",
    "def process(txt):\n",
    "    txt1 = re.sub('[\\n]',\" \",txt)\n",
    "    txt1 = re.sub('[^A-Za-z ]+', '', txt1)\n",
    "    \n",
    "    _wrds = txt1.split()\n",
    "    stemmer = SnowballStemmer(\"english\") ## May use porter stemmer\n",
    "    wrds = [stemmer.stem(wrd) for wrd in _wrds]\n",
    "    return wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that processes raw string and gets a processes list containing lines'''\n",
    "def get_processed_sentences(snt_txt):\n",
    "    snt_list = []\n",
    "    for line in snt_txt.split('.'):\n",
    "        line = line.strip()\n",
    "        if len(line.split()) >= 5:\n",
    "            snt_list.append(line)\n",
    "    return snt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method extracts topic from sentence'''\n",
    "def extract_topic(str_arg, num_topics = 1, num_top_words = 3):\n",
    "    vectorizer = text.CountVectorizer(input='content', analyzer='word', lowercase=True, stop_words='english')\n",
    "    try:\n",
    "        dtm = vectorizer.fit_transform(str_arg.split())\n",
    "        vocab = np.array(vectorizer.get_feature_names())\n",
    "    \n",
    "        #clf = decomposition.NMF(n_components=num_topics, random_state=1) ## topic extraction\n",
    "        clf = decomposition.LatentDirichletAllocation(n_components=num_topics, learning_method='online')\n",
    "        clf.fit_transform(dtm)\n",
    "\n",
    "        topic_words = []\n",
    "        for topic in clf.components_:\n",
    "            word_idx = np.argsort(topic)[::-1][0:num_top_words] ##[::-1] reverses the list\n",
    "            topic_words.append([vocab[i] for i in word_idx])\n",
    "        return topic_words\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method extracts topics of each sentence and returns a list'''\n",
    "def extract_topics_all(doc_string):\n",
    "    #One entry per sentence in list\n",
    "    doc_str = process_text(doc_string)\n",
    "    doc_str = get_processed_sentences(doc_str)\n",
    "    \n",
    "    res = []\n",
    "    for i in range (0, len(doc_str)):\n",
    "        snd_str = doc_str[i].lower()\n",
    "        #print(\"Sending ----------------------------\",snd_str,\"==========\",len(snd_str))\n",
    "        tmp_topic = extract_topic(snd_str, num_topics = 2, num_top_words = 1)\n",
    "        for top in tmp_topic:\n",
    "            for wrd in top:\n",
    "                res.append(wrd)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a dataframe and returns all the topics in the entire corpus'''\n",
    "def extract_corpus_topics(arg_df):\n",
    "    all_topics = set()\n",
    "    cnt = 1\n",
    "    for txt in arg_df:\n",
    "        all_topics = all_topics.union(extract_topics_all(txt))\n",
    "        print(\"Processed \",cnt,\" records\")\n",
    "        cnt += 1\n",
    "    all_topics = list(all_topics)\n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET A VECTORIZED REPRESENTATION OF ALL THE TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data_set = words list per document.\n",
    "    vocabulary = list of all the words present\n",
    "    _vocab = dict of word counts for words in vocabulary'''\n",
    "def get_vocab_wrd_map(df_text):\n",
    "    data_set = []\n",
    "    vocabulary = []\n",
    "    _vocab = defaultdict(int)\n",
    "    for i in range(0,df_text.size):\n",
    "        txt = process(df_text[i])\n",
    "        data_set.append(txt)\n",
    "\n",
    "        for wrd in txt:\n",
    "            _vocab[wrd] += 1\n",
    "\n",
    "        vocabulary = vocabulary + txt\n",
    "        vocabulary = list(set(vocabulary))\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(\"%5d records processed\"%(i))\n",
    "    return data_set, vocabulary, _vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''vocab = return sorted list of most common words in vocabulary'''\n",
    "def get_common_vocab(num_arg, vocab):\n",
    "    vocab = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    vocab = vocab[:num_arg]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert vocabulary and most common words to map for faster access'''\n",
    "def get_vocab_map(vocabulary, vocab):\n",
    "    vocab_map = {}\n",
    "    for i in range(0,len(vocab)):\n",
    "        vocab_map[vocab[i][0]] = i \n",
    "    \n",
    "    vocabulary_map = {}\n",
    "    for i in range(0,len(vocabulary)):\n",
    "        vocabulary_map[vocabulary[i]] = i\n",
    "    \n",
    "    return vocabulary_map, vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function returns n-gram context embedding for each word'''\n",
    "def get_embedding(word, data_set, vocab_map, wdw_size):\n",
    "    embedding = [0]*len(vocab_map)\n",
    "    for docs in data_set:\n",
    "        for i in range(wdw_size, len(docs)-wdw_size):\n",
    "            if docs[i] == word:\n",
    "                for j in range(i-wdw_size, i-1):\n",
    "                    if docs[j] in vocab_map:\n",
    "                        embedding[vocab_map[docs[j]]] += 1\n",
    "                for j in range(i+1, i+wdw_size):\n",
    "                    if docs[j] in vocab_map:\n",
    "                        embedding[vocab_map[docs[j]]] += 1\n",
    "    total_words = sum(embedding)\n",
    "    if total_words != 0:\n",
    "        embedding[:] = [e/total_words for e in embedding]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a helper function that returns n-gram embedding for all the topics in the corpus'''\n",
    "def get_embedding_all(all_topics, data_set, vocab_map, wdw_size):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(all_topics)):\n",
    "        embeddings.append(get_embedding(all_topics[i], data_set, vocab_map, wdw_size))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_matrix_multiplication(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using matrix vector multiplication.\n",
    "    \"\"\"\n",
    "    dotted = matrix.dot(vector)\n",
    "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function generates most similar topic to a given embedding'''\n",
    "def get_most_similar_topics(embd, embeddings, all_topics, num_wrd=10):\n",
    "    sim_top = []\n",
    "    cos_sim = cos_matrix_multiplication(np.array(embeddings), embd)\n",
    "    closest_match = cos_sim.argsort()[-num_wrd:][::-1]\n",
    "    for i in range(0, closest_match.shape[0]):\n",
    "        sim_top.append(all_topics[closest_match[i]])\n",
    "    return sim_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function extracts matches for a regular expression in the text'''\n",
    "def get_regex_match(regex, str_arg):\n",
    "    srch = re.search(regex,str_arg)\n",
    "    if srch is not None:\n",
    "        return srch.group(0).strip()\n",
    "    else:\n",
    "        return \"Not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a helper function that helps extracting answer to extraction type questions'''\n",
    "def extract(key,str_arg):\n",
    "    if key == 'dob':\n",
    "        return get_regex_match('Date of Birth:(.*)] ', str_arg)\n",
    "    elif key == 'a_date':\n",
    "        return get_regex_match('Admission Date:(.*)] ', str_arg)\n",
    "    elif key == 'd_date':\n",
    "        return get_regex_match('Discharge Date:(.*)]\\n', str_arg)\n",
    "    elif key == 'sex':\n",
    "        return get_regex_match('Sex:(.*)\\n', str_arg)\n",
    "    elif key == 'service':\n",
    "        return get_regex_match('Service:(.*)\\n', str_arg)\n",
    "    elif key == 'allergy':\n",
    "        return get_regex_match('Allergies:(.*)\\n(.*)\\n', str_arg)\n",
    "    elif key == 'attdng':\n",
    "        return get_regex_match('Attending:(.*)]\\n', str_arg)\n",
    "    else:\n",
    "        return \"I Don't know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method extracts topic from sentence'''\n",
    "def extract_topic(str_arg, num_topics = 1, num_top_words = 3):\n",
    "    vectorizer = text.CountVectorizer(input='content', analyzer='word', lowercase=True, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(str_arg.split())\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    \n",
    "    #clf = decomposition.NMF(n_components=num_topics, random_state=1) ## topic extraction\n",
    "    clf = decomposition.LatentDirichletAllocation(n_components=num_topics, learning_method='online')\n",
    "    clf.fit_transform(dtm)\n",
    "    \n",
    "    topic_words = []\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words] ##[::-1] reverses the list\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method extracts topics in a question'''\n",
    "def extract_Q_topic(str_arg):\n",
    "    try:\n",
    "        return extract_topic(str_arg)\n",
    "    except:\n",
    "        return None\n",
    "    ## Future Scope fix later for more comprehensive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extract_map(key_wrd):\n",
    "    ## A Stemmed mapping for simple extractions\n",
    "    extract_map = {'birth':'dob', 'dob':'dob',\n",
    "              'admiss':'a_date', 'discharg':'d_date',\n",
    "              'sex':'sex', 'gender':'sex', 'servic':'service',\n",
    "              'allergi':'allergy', 'attend':'attdng'}\n",
    "    if key_wrd in extract_map.keys():\n",
    "        return extract_map[key_wrd]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Method that generates the answer for text extraction questions'''\n",
    "def get_extracted_answer(topic_str, text):\n",
    "    port = PorterStemmer()\n",
    "    for i in range(0, len(topic_str)):\n",
    "        rel_wrd = topic_str[i]\n",
    "        for wrd in rel_wrd:\n",
    "            key = get_extract_map(port.stem(wrd))\n",
    "            if key is not None:\n",
    "                return extract(key, text)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This method extracts topics of each sentence and returns a list'''\n",
    "def get_topic_mapping(doc_string):\n",
    "    #One entry per sentence in list\n",
    "    doc_str = process_text(doc_string)\n",
    "    doc_str = get_processed_sentences(doc_str)\n",
    "    \n",
    "    res = defaultdict(list)\n",
    "    for i in range (0, len(doc_str)):\n",
    "        snd_str = doc_str[i].lower()\n",
    "        #print(\"Sending ----------------------------\",snd_str,\"==========\",len(snd_str))\n",
    "        tmp_topic = extract_topic(snd_str, num_topics = 2, num_top_words = 1)\n",
    "        for top in tmp_topic:\n",
    "            for wrd in top:\n",
    "                res[wrd].append(doc_str[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direct_answer(topic_str, topic_map):\n",
    "    ## Maybe apply lemmatizer here\n",
    "    for i in range(0, len(topic_str)):\n",
    "        rel_wrd = topic_str[i]\n",
    "        for wrd in rel_wrd:\n",
    "            if wrd in topic_map.keys():\n",
    "                return topic_map[wrd]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(topic, topic_map, embedding_short, all_topics, data_set, vocab_map, pca, wdw_size=5):\n",
    "    ## Get most similar topics\n",
    "    tpc_embedding = get_embedding(topic, data_set, vocab_map, wdw_size)\n",
    "    tpc_embedding = pca.transform([tpc_embedding])\n",
    "    sim_topics = get_most_similar_topics(tpc_embedding[0], embedding_short, all_topics, num_wrd = len(all_topics))\n",
    "    for topic in sim_topics:\n",
    "        if topic in topic_map.keys():\n",
    "            return topic_map[topic]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function checks if the user input text is an instruction allowed in chatbot or not'''\n",
    "def is_instruction_option(str_arg):\n",
    "    if str_arg == \"exit\" or str_arg == \"summary\" or str_arg == \"reveal\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ... \n",
      "\n",
      "Getting Vocabulary ...\n",
      "    0 records processed\n",
      "Creating context ...\n",
      "Learning topics ...\n",
      "Processed  1  records\n",
      "Processed  2  records\n",
      "Processed  3  records\n",
      "Processed  4  records\n",
      "Processed  5  records\n",
      "Processed  6  records\n",
      "Processed  7  records\n",
      "Processed  8  records\n",
      "Processed  9  records\n",
      "Processed  10  records\n",
      "Processed  11  records\n",
      "Processed  12  records\n",
      "Processed  13  records\n",
      "Processed  14  records\n",
      "Processed  15  records\n",
      "Processed  16  records\n",
      "Processed  17  records\n",
      "Processed  18  records\n",
      "Processed  19  records\n",
      "Processed  20  records\n",
      "Processed  21  records\n",
      "Processed  22  records\n",
      "Processed  23  records\n",
      "Processed  24  records\n",
      "Processed  25  records\n",
      "Processed  26  records\n",
      "Processed  27  records\n",
      "Processed  28  records\n",
      "Processed  29  records\n",
      "Processed  30  records\n",
      "Processed  31  records\n",
      "Processed  32  records\n",
      "Processed  33  records\n",
      "Processed  34  records\n",
      "Processed  35  records\n",
      "Processed  36  records\n",
      "Processed  37  records\n",
      "Processed  38  records\n",
      "Processed  39  records\n",
      "Processed  40  records\n",
      "Processed  41  records\n",
      "Processed  42  records\n",
      "Processed  43  records\n",
      "Processed  44  records\n",
      "Processed  45  records\n",
      "Processed  46  records\n",
      "Processed  47  records\n",
      "Processed  48  records\n",
      "Processed  49  records\n",
      "Processed  50  records\n",
      "Getting Embeddings\n",
      "Bot:> I am online!\n",
      "Bot:> Type \"exit\" to switch to end a patient's session\n",
      "Bot:> Type \"summary\" to view patient's discharge summary\n",
      "Bot:> What is your Patient Id [0 to 49?]5\n",
      "Bot:> Reading Discharge Summary for Patient Id:  5\n",
      "Bot:> How can I help ?\n",
      "Person:>What is my date of birth?\n",
      "Bot:>  Date of Birth:  [**2109-10-8**]\n",
      "Bot:> How can I help ?\n",
      "Person:>What is my admission date?\n",
      "Bot:>  Admission Date:  [**2172-3-5**]\n",
      "Bot:> How can I help ?\n",
      "Person:>When was I discharged?\n",
      "Bot:>  Discharge Date:   [**2172-3-8**]\n",
      "Bot:> How can I help ?\n",
      "Person:>What is my gender?\n",
      "Bot:>  Sex:   F\n",
      "Bot:> How can I help ?\n",
      "Person:>What are the services I had?\n",
      "Bot:>  Service: NEUROSURGERY\n",
      "Bot:> How can I help ?\n",
      "Person:>Do I have allergy?\n",
      "Bot:>  Allergies:\n",
      "No Known Allergies / Adverse Drug Reactions\n",
      "Bot:> How can I help ?\n",
      "Person:>Who was my attending?\n",
      "Bot:>  Attending:[**First Name3 (LF) 1835**]\n",
      "Bot:> How can I help ?\n",
      "Person:>Am I married?\n",
      "Bot:>  ['She was found to hve a right cavernous sinus and nasopharyngeal mass', 'She cam to the BTC for discussion about management of her right cavernous sinus mass that extends into the middle cranial fossa', 'A gadoliniumenhanced head MRI performed at Hospital  on  showed a bright mass involving the cavernous sinus']\n",
      "Bot:> How can I help ?\n",
      "Person:>reveal\n",
      "defaultdict(<class 'list'>, {'known': ['Admission Date                Discharge Date     Date of Birth               Sex   F  Service NEUROSURGERY  Allergies No Known Allergies  Adverse Drug Reactions  AttendingFirst Name LF  Chief Complaint Meningioma  Major Surgical or Invasive Procedure Right Craniotomy   History of Present Illness Known firstname  Known lastname  is a yearold woman with longstanding history of rheumatoid arthritis probable Sweets syndrome and multiple joint complications requiring orthopedic interventions'], 'date': ['Admission Date                Discharge Date     Date of Birth               Sex   F  Service NEUROSURGERY  Allergies No Known Allergies  Adverse Drug Reactions  AttendingFirst Name LF  Chief Complaint Meningioma  Major Surgical or Invasive Procedure Right Craniotomy   History of Present Illness Known firstname  Known lastname  is a yearold woman with longstanding history of rheumatoid arthritis probable Sweets syndrome and multiple joint complications requiring orthopedic interventions'], 'sinus': ['She was found to hve a right cavernous sinus and nasopharyngeal mass'], 'cavernous': ['She was found to hve a right cavernous sinus and nasopharyngeal mass', 'She cam to the BTC for discussion about management of her right cavernous sinus mass that extends into the middle cranial fossa', 'A gadoliniumenhanced head MRI performed at Hospital  on  showed a bright mass involving the cavernous sinus'], 'underwent': ['She underwent a biopsy of hte nasopharyngeal mass by Dr'], 'dr': ['She underwent a biopsy of hte nasopharyngeal mass by Dr'], 'reactive': ['First Name NamePattern  Last Name NamePattern  and the pathology including flow cytometry was reactive for Tcell lymphoid hyperplasia only'], 'namepattern': ['First Name NamePattern  Last Name NamePattern  and the pathology including flow cytometry was reactive for Tcell lymphoid hyperplasia only'], 'history': ['She has a longstanding history of rheumatoid arthritis that involved small and large joints in her body', 'Please refer additional past medical history past surgical history facial history and social history to the initial note on', 'Social History She is married'], 'large': ['She has a longstanding history of rheumatoid arthritis that involved small and large joints in her body'], 'currently': ['Her disease is currently controlled by abatacept hydroxychloroquine and methotrexate'], 'hydroxychloroquine': ['Her disease is currently controlled by abatacept hydroxychloroquine and methotrexate'], 'headache': ['She also has a remote history of erythematous nodules at her shins dermatosis probable Sweets disease severe holocranial headache with an intensity of  and dysphagia', 'She had fullness in her ear and she also had a cold coinciding to the onset of her headache', 'Pain or headache that is continually increasing or not relieved by pain medication'], 'nodules': ['She also has a remote history of erythematous nodules at her shins dermatosis probable Sweets disease severe holocranial headache with an intensity of  and dysphagia'], 'treatment': ['But her symptoms resolved with treatment for autoimmune disease'], 'resolved': ['But her symptoms resolved with treatment for autoimmune disease'], 'surgical': ['Please refer additional past medical history past surgical history facial history and social history to the initial note on'], 'fossa': ['She cam to the BTC for discussion about management of her right cavernous sinus mass that extends into the middle cranial fossa'], 'hospital': ['She had a recent head CT at the Hospital  and Womans Hospital on  when she went for a consultation there', 'A gadoliniumenhanced head MRI performed at Hospital  on  showed a bright mass involving the cavernous sinus'], 'womans': ['She had a recent head CT at the Hospital  and Womans Hospital on  when she went for a consultation there'], 'vomiting': ['She is neurologically stable without headache nausea vomiting seizure imbalance or fall'], 'fall': ['She is neurologically stable without headache nausea vomiting seizure imbalance or fall', 'She did not have nausea vomiting blurry vision imbalance or fall'], 'systemic': ['She has no new systemic complaints'], 'complaints': ['She has no new systemic complaints'], 'started': ['Her neurological problem started  when she experienced frontal pressurelike sensations'], 'problem': ['Her neurological problem started  when she experienced frontal pressurelike sensations'], 'evening': ['There was no temporal pattern but they may occur more often in the evening'], 'pattern': ['There was no temporal pattern but they may occur more often in the evening'], 'cold': ['She had fullness in her ear and she also had a cold coinciding to the onset of her headache'], 'late': ['By late Month only  and early  she also developed a sharp pain intermittently in the right temple region'], 'temple': ['By late Month only  and early  she also developed a sharp pain intermittently in the right temple region'], 'vision': ['She did not have nausea vomiting blurry vision imbalance or fall'], 'hip': ['Past Medical History She has a history of rheumatoid arthritis unspecified dermatosis right knee replacement left hip replacement and fusion of subtalar joint and resection of a benign left parotid gland tumor'], 'left': ['Past Medical History She has a history of rheumatoid arthritis unspecified dermatosis right knee replacement left hip replacement and fusion of subtalar joint and resection of a benign left parotid gland tumor'], 'social': ['Social History She is married'], 'approximately': ['She had smoked for approximately a year and a half when she was younger but is not currently smoking'], 'smoked': ['She had smoked for approximately a year and a half when she was younger but is not currently smoking'], 'glass': ['She has approximately one glass of wine per week', 'Make sure to take your steroid medication with meals or a glass of milk'], 'wine': ['She has approximately one glass of wine per week'], 'teacher': ['She is retired but was employed as a teacher'], 'retired': ['She is retired but was employed as a teacher'], 'heart': ['Family History Cancer diabetes hearing loss and heart disease'], 'hearing': ['Family History Cancer diabetes hearing loss and heart disease'], 'lnn': ['Physical Exam AF VSS HEENT normal no LNN Neck supple'], 'supple': ['Physical Exam AF VSS HEENT normal no LNN Neck supple'], 'examination': ['RRR CTA NTTP warm peripherals  Neurological Examination  Her Karnofsky Performance Score is'], 'performance': ['RRR CTA NTTP warm peripherals  Neurological Examination  Her Karnofsky Performance Score is'], 'mri': ['Pertinent Results MRI  Right middle cranial fossa mass likely represents a meningioma and is stable since MRI of'], 'right': ['Pertinent Results MRI  Right middle cranial fossa mass likely represents a meningioma and is stable since MRI of'], 'mass': ['The previously seen midline nasopharyngeal mass has decreased in size since MRI of', 'Direct visual inspection would be helpful for further assessment of the nasopharyngeal mass'], 'previously': ['The previously seen midline nasopharyngeal mass has decreased in size since MRI of'], 'nasopharyngeal': ['Direct visual inspection would be helpful for further assessment of the nasopharyngeal mass'], 'meningioma': ['Brief Hospital Course Patient presented electively for meningioma resection of', 'Prelim path is consistent with meningioma'], 'resection': ['Brief Hospital Course Patient presented electively for meningioma resection of'], 'room': ['She tolerated the procedure well and was extubated in the operating room'], 'operating': ['She tolerated the procedure well and was extubated in the operating room'], 'icu': ['She was trasnported to the ICU postoperatively for management'], 'trasnported': ['She was trasnported to the ICU postoperatively for management'], 'hours': ['She had no complications and was transferred to the floor and observed for  hours', 'mg Tablet Sig Four  Tablet PO qh  for  days take  tabs every  hours on  and take  tabs every  hours on  then stop'], 'complications': ['She had no complications and was transferred to the floor and observed for  hours'], 'path': ['Prelim path is consistent with meningioma'], 'clinic': ['She has dissolvable sutures and will need to come to neurosurgery clinic in  days for wound check only'], 'neurosurgery': ['She has dissolvable sutures and will need to come to neurosurgery clinic in  days for wound check only'], 'brain': ['She will need to be scheduled for brain tumor clinic', 'Disp Tablets Refills   Discharge Disposition Home  Discharge Diagnosis brain lesion  Discharge Condition Mental Status Clear and coherent'], 'need': ['She will need to be scheduled for brain tumor clinic'], 'maintenance': ['She will complete Decadron taper on  and then restart her maintenance dose of prednisone'], 'restart': ['She will complete Decadron taper on  and then restart her maintenance dose of prednisone'], 'keppra': ['She will also be taking Keppra for seizure prophlyaxis'], 'prophlyaxis': ['She will also be taking Keppra for seizure prophlyaxis'], 'deficits': ['Her neurologic examination was intact with no deficits at discharge'], 'discharge': ['Her neurologic examination was intact with no deficits at discharge', 'Disp Tablets Refills   Discharge Disposition Home  Discharge Diagnosis brain lesion  Discharge Condition Mental Status Clear and coherent'], 'tolerating': ['She was tolerating regular diet'], 'regular': ['She was tolerating regular diet'], 'counter': ['She should continue to take over the counter laxatives as needed'], 'continue': ['She should continue to take over the counter laxatives as needed'], 'bactrim': ['Medications on Admission bactrim famotidinefolic acid fosamax lorezapam methotrexate mvi orencia plaquenil prednisone qd   Discharge Medications'], 'medications': ['Medications on Admission bactrim famotidinefolic acid fosamax lorezapam methotrexate mvi orencia plaquenil prednisone qd   Discharge Medications'], 'tablet': ['hydroxychloroquine  mg Tablet Sig One  Tablet PO DAILY Daily', 'levetiracetam  mg Tablet Sig One  Tablet PO BID  times a day', 'prednisone  mg Tablet Sig Two  Tablet PO DAILY Daily start the day after Decadron taper is complete', 'famotidine  mg Tablet Sig One  Tablet PO BID  times a day'], 'daily': ['hydroxychloroquine  mg Tablet Sig One  Tablet PO DAILY Daily', 'prednisone  mg Tablet Sig Two  Tablet PO DAILY Daily start the day after Decadron taper is complete'], 'po': ['levetiracetam  mg Tablet Sig One  Tablet PO BID  times a day'], 'tabs': ['mg Tablet Sig Four  Tablet PO qh  for  days take  tabs every  hours on  and take  tabs every  hours on  then stop'], 'day': ['famotidine  mg Tablet Sig One  Tablet PO BID  times a day'], 'alert': ['Level of Consciousness Alert and interactive'], 'consciousness': ['Level of Consciousness Alert and interactive'], 'signs': ['Discharge Instructions Have a friendfamily member check your incision daily for signs of infection'], 'friendfamily': ['Discharge Instructions Have a friendfamily member check your incision daily for signs of infection'], 'medicine': ['Take your pain medicine as prescribed'], 'prescribed': ['Take your pain medicine as prescribed', 'If you have been prescribed Dilantin Phenytoin for antiseizure medicine take it as prescribed and follow up with laboratory blood drawing in one week'], 'straining': ['Exercise should be limited to walking no lifting straining or excessive bending'], 'exercise': ['Exercise should be limited to walking no lifting straining or excessive bending'], 'sutures': ['You have dissolvable sutures you must keep that area dry for  days'], 'dry': ['You have dissolvable sutures you must keep that area dry for  days'], 'shower': ['You may shower before this time using a shower cap to cover your head'], 'cover': ['You may shower before this time using a shower cap to cover your head'], 'fiber': ['Increase your intake of fluids and fiber as narcotic pain medicine can cause constipation'], 'intake': ['Increase your intake of fluids and fiber as narcotic pain medicine can cause constipation'], 'narcotic': ['We generally recommend taking an over the counter stool softener such as Docusate Colace while taking narcotic pain medication'], 'taking': ['We generally recommend taking an over the counter stool softener such as Docusate Colace while taking narcotic pain medication'], 'medicines': ['Unless directed by your doctor do not take any antiinflammatory medicines such as Motrin Aspirin Advil and Ibuprofen etc'], 'ibuprofen': ['Unless directed by your doctor do not take any antiinflammatory medicines such as Motrin Aspirin Advil and Ibuprofen etc'], 'week': ['If you have been prescribed Dilantin Phenytoin for antiseizure medicine take it as prescribed and follow up with laboratory blood drawing in one week'], 'results': ['This can be drawn at your PCPs office but please have the results faxed to TelephoneFax'], 'telephonefax': ['This can be drawn at your PCPs office but please have the results faxed to TelephoneFax'], 'require': ['If you have been discharged on Keppra Levetiracetam you will not require blood work monitoring'], 'work': ['If you have been discharged on Keppra Levetiracetam you will not require blood work monitoring', 'Clearance to drive and return to work will be addressed at your postoperative office visit'], 'medication': ['If you are being sent home on steroid medication make sure you are taking a medication to protect your stomach Prilosec Protonix or Pepcid as these medications can cause stomach irritation', 'Make sure to take your steroid medication with meals or a glass of milk'], 'stomach': ['If you are being sent home on steroid medication make sure you are taking a medication to protect your stomach Prilosec Protonix or Pepcid as these medications can cause stomach irritation'], 'drive': ['Clearance to drive and return to work will be addressed at your postoperative office visit'], 'incentive': ['Make sure to continue to use your incentive spirometer while at home'], 'make': ['Make sure to continue to use your incentive spirometer while at home', 'Please make this appointment by calling TelephoneFax'], 'new': ['CALL YOUR SURGEON IMMEDIATELY IF YOU EXPERIENCE ANY OF THE FOLLOWING  New onset of tremors or seizures'], 'following': ['CALL YOUR SURGEON IMMEDIATELY IF YOU EXPERIENCE ANY OF THE FOLLOWING  New onset of tremors or seizures'], 'mental': ['Any confusion or change in mental status'], 'change': ['Any confusion or change in mental status'], 'extremities': ['Any numbness tingling weakness in your extremities'], 'weakness': ['Any numbness tingling weakness in your extremities'], 'pain': ['Pain or headache that is continually increasing or not relieved by pain medication'], 'increased': ['Any signs of infection at the wound site increasing redness increased swelling increased tenderness or drainage'], 'redness': ['Any signs of infection at the wound site increasing redness increased swelling increased tenderness or drainage'], 'equal': ['Fever greater than or equal to  F'], 'fever': ['Fever greater than or equal to  F'], 'wound': ['Followup Instructions Please return to the office in  days from your date of surgery for a wound check'], 'office': ['Followup Instructions Please return to the office in  days from your date of surgery for a wound check', 'If you live quite a distance from our office please make arrangements for the same with your PCP'], 'nurse': ['This appointment can be made with the Nurse Practitioner'], 'practitioner': ['This appointment can be made with the Nurse Practitioner'], 'calling': ['Please make this appointment by calling TelephoneFax'], 'arrangements': ['If you live quite a distance from our office please make arrangements for the same with your PCP']}) dict_keys(['known', 'date', 'sinus', 'cavernous', 'underwent', 'dr', 'reactive', 'namepattern', 'history', 'large', 'currently', 'hydroxychloroquine', 'headache', 'nodules', 'treatment', 'resolved', 'surgical', 'fossa', 'hospital', 'womans', 'vomiting', 'fall', 'systemic', 'complaints', 'started', 'problem', 'evening', 'pattern', 'cold', 'late', 'temple', 'vision', 'hip', 'left', 'social', 'approximately', 'smoked', 'glass', 'wine', 'teacher', 'retired', 'heart', 'hearing', 'lnn', 'supple', 'examination', 'performance', 'mri', 'right', 'mass', 'previously', 'nasopharyngeal', 'meningioma', 'resection', 'room', 'operating', 'icu', 'trasnported', 'hours', 'complications', 'path', 'clinic', 'neurosurgery', 'brain', 'need', 'maintenance', 'restart', 'keppra', 'prophlyaxis', 'deficits', 'discharge', 'tolerating', 'regular', 'counter', 'continue', 'bactrim', 'medications', 'tablet', 'daily', 'po', 'tabs', 'day', 'alert', 'consciousness', 'signs', 'friendfamily', 'medicine', 'prescribed', 'straining', 'exercise', 'sutures', 'dry', 'shower', 'cover', 'fiber', 'intake', 'narcotic', 'taking', 'medicines', 'ibuprofen', 'week', 'results', 'telephonefax', 'require', 'work', 'medication', 'stomach', 'drive', 'incentive', 'make', 'new', 'following', 'mental', 'change', 'extremities', 'weakness', 'pain', 'increased', 'redness', 'equal', 'fever', 'wound', 'office', 'nurse', 'practitioner', 'calling', 'arrangements'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:> How can I help ?\n",
      "Person:>How many tablet should I take?\n",
      "Bot:>  ['hydroxychloroquine  mg Tablet Sig One  Tablet PO DAILY Daily', 'levetiracetam  mg Tablet Sig One  Tablet PO BID  times a day', 'prednisone  mg Tablet Sig Two  Tablet PO DAILY Daily start the day after Decadron taper is complete', 'famotidine  mg Tablet Sig One  Tablet PO BID  times a day']\n",
      "Bot:> How can I help ?\n",
      "Person:>How was my MRI?\n",
      "Bot:>  ['Pertinent Results MRI  Right middle cranial fossa mass likely represents a meningioma and is stable since MRI of']\n",
      "Bot:> How can I help ?\n",
      "Person:>What do I do if I have seizures?\n",
      "Bot:>  ['She was found to hve a right cavernous sinus and nasopharyngeal mass', 'She cam to the BTC for discussion about management of her right cavernous sinus mass that extends into the middle cranial fossa', 'A gadoliniumenhanced head MRI performed at Hospital  on  showed a bright mass involving the cavernous sinus']\n",
      "Bot:> How can I help ?\n",
      "Person:>How is my knee?\n",
      "Bot:>  ['She was found to hve a right cavernous sinus and nasopharyngeal mass', 'She cam to the BTC for discussion about management of her right cavernous sinus mass that extends into the middle cranial fossa', 'A gadoliniumenhanced head MRI performed at Hospital  on  showed a bright mass involving the cavernous sinus']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data ...\",\"\\n\")\n",
    "    df_text = load_data()\n",
    "    \n",
    "    print(\"Getting Vocabulary ...\")\n",
    "    data_set, vocabulary, _vocab = get_vocab_wrd_map(df_text)\n",
    "    \n",
    "    print(\"Creating context ...\")\n",
    "    vocab = get_common_vocab(1000, _vocab)\n",
    "    vocabulary_map, vocab_map = get_vocab_map(vocabulary, vocab)\n",
    "    \n",
    "    print(\"Learning topics ...\")\n",
    "    all_topics = extract_corpus_topics(df_text)\n",
    "    \n",
    "    print(\"Getting Embeddings\")\n",
    "    embeddings = get_embedding_all(all_topics, data_set, vocab_map, 5)\n",
    "    \n",
    "    pca = PCA(n_components=10)\n",
    "    embedding_short = pca.fit_transform(embeddings)\n",
    "    \n",
    "    print(\"Bot:> I am online!\")\n",
    "    print(\"Bot:> Type \\\"exit\\\" to switch to end a patient's session\")\n",
    "    print(\"Bot:> Type \\\"summary\\\" to view patient's discharge summary\")\n",
    "    while(True):\n",
    "        while(True):\n",
    "            try:\n",
    "                pid = int(input(\"Bot:> What is your Patient Id [0 to \"+str(df_text.shape[0]-1)+\"?]\"))\n",
    "            except:\n",
    "                continue\n",
    "            if pid < 0 or pid > df_text.shape[0]-1:\n",
    "                print(\"Bot:> Patient Id out or range!\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Bot:> Reading Discharge Summary for Patient Id: \",pid)\n",
    "                break\n",
    "\n",
    "        personal_topics = extract_topics_all(df_text[pid])\n",
    "        topic_mapping = get_topic_mapping(df_text[pid])\n",
    "        \n",
    "        ques = \"random starter\"\n",
    "        while(ques != \"exit\"):\n",
    "            ## Read Question\n",
    "            ques = input(\"Bot:> How can I help ?\\nPerson:>\")\n",
    "            \n",
    "            ## Check if it is an instructional question\n",
    "            if is_instruction_option(ques):\n",
    "                if ques == \"summary\":\n",
    "                    print(\"Bot:> ================= Discharge Summary for Patient Id \",pid,\"\\n\")\n",
    "                    print(df_text[pid])\n",
    "                elif ques == \"reveal\":\n",
    "                    print(topic_mapping, topic_mapping.keys())\n",
    "                continue\n",
    "                \n",
    "            ## Extract Question topic\n",
    "            topic_q = extract_Q_topic(ques)\n",
    "            if topic_q is None:\n",
    "                print(\"Bot:> I am a specialized NLP bot, please as a more specific question for me!\")\n",
    "                continue\n",
    "            ans = get_extracted_answer(topic_q, df_text[pid])\n",
    "            if ans is not None:\n",
    "                print(\"Bot:> \",ans)\n",
    "            else:\n",
    "                ans = get_direct_answer(topic_q, topic_mapping)\n",
    "                if ans is not None:\n",
    "                    print(\"Bot:> \",ans)\n",
    "                else:\n",
    "                    ans = get_answer(topic_q, topic_mapping, embedding_short, all_topics, data_set, vocab_map, pca, 5)\n",
    "                    if ans is not None:\n",
    "                        print(\"Bot:> \",ans)\n",
    "                    else:\n",
    "                        print(\"Bot:> Sorry but, I have no information on this topic!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
