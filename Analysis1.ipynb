{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:57:46.173142Z",
     "start_time": "2018-05-16T16:57:19.787937Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:13.355831Z",
     "start_time": "2018-05-16T16:43:13.349814Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = r'D:\\ORGANIZATION\\UCSD_Life\\Work\\4. Quarter-3\\Subjects\\MED 277\\Project\\DATA\\\\'\n",
    "data_file = base_path+\"NOTEEVENTS.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(data_file, compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n",
    "df = pd.read_csv(data_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking small data for analysis\n",
    "df1 = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(df1,base_path+'data10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:31.291975Z",
     "start_time": "2018-05-16T16:43:28.755831Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 =  joblib.load(base_path+'data10.pkl')\n",
    "df = df1[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:33.292801Z",
     "start_time": "2018-05-16T16:43:33.005751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
       "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
       "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
       "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
       "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "0  Discharge summary      Report   NaN      NaN   \n",
       "1  Discharge summary      Report   NaN      NaN   \n",
       "2  Discharge summary      Report   NaN      NaN   \n",
       "3  Discharge summary      Report   NaN      NaN   \n",
       "4  Discharge summary      Report   NaN      NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:36.241233Z",
     "start_time": "2018-05-16T16:43:36.231709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:39.697091Z",
     "start_time": "2018-05-16T16:43:39.664513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Discharge summary'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:42.574457Z",
     "start_time": "2018-05-16T16:43:42.567391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Report'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DESCRIPTION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:45.079783Z",
     "start_time": "2018-05-16T16:43:45.004015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Converting object to datetime\n",
    "df['CHARTDATE'] =  pd.to_datetime(df['CHARTDATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size includes NAN values count does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Null=\",df['STORETIME'].isnull().sum(),\" out of total \",df['STORETIME'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:43:54.011604Z",
     "start_time": "2018-05-16T16:43:53.987039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ROW_ID \t has \t NUll =          0 out of total \t 50 \n",
      "SUBJECT_ID \t has \t NUll =          0 out of total \t 50 \n",
      "   HADM_ID \t has \t NUll =          0 out of total \t 50 \n",
      " CHARTDATE \t has \t NUll =          0 out of total \t 50 \n",
      " CHARTTIME \t has \t NUll =         50 out of total \t 50 \n",
      " STORETIME \t has \t NUll =         50 out of total \t 50 \n",
      "  CATEGORY \t has \t NUll =          0 out of total \t 50 \n",
      "DESCRIPTION \t has \t NUll =          0 out of total \t 50 \n",
      "      CGID \t has \t NUll =         50 out of total \t 50 \n",
      "   ISERROR \t has \t NUll =         50 out of total \t 50 \n",
      "      TEXT \t has \t NUll =          0 out of total \t 50 \n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(\"%10s \\t has \\t NUll = %10d out of total \\t %d \"%(col, df[col].size - df[col].count(),df[col].size))\n",
    "    #print(col,\" Null =\",df[col].size - df[col].count(),\" out of total \",df[col].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:44:01.295860Z",
     "start_time": "2018-05-16T16:44:01.255244Z"
    }
   },
   "outputs": [],
   "source": [
    "df_text = df[['CHARTDATE','CATEGORY','DESCRIPTION','TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:44:04.149606Z",
     "start_time": "2018-05-16T16:44:04.142062Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(df_text.CATEGORY[idx])\n",
    "print(df_text.DESCRIPTION[idx])\n",
    "print(df_text.TEXT[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:48:33.292096Z",
     "start_time": "2018-05-16T16:48:33.285077Z"
    }
   },
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:45:51.591272Z",
     "start_time": "2018-05-16T16:45:51.585259Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = df_text.TEXT[idx]\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:50:13.474498Z",
     "start_time": "2018-05-16T16:50:13.467491Z"
    }
   },
   "outputs": [],
   "source": [
    "#re.sub('[^A-Za-z0-9 ]+', '', txt)\n",
    "txt1 = re.sub('[\\n]',\" \",txt)\n",
    "txt1 = re.sub('[^A-Za-z ]+', '', txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:00:54.069671Z",
     "start_time": "2018-05-16T17:00:54.062518Z"
    }
   },
   "outputs": [],
   "source": [
    "_wrds = txt1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:02:06.623718Z",
     "start_time": "2018-05-16T17:02:06.618125Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(_wrds))\n",
    "print(_wrds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:57:53.268985Z",
     "start_time": "2018-05-16T16:57:53.255950Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/howto/stem.html\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T16:58:27.620524Z",
     "start_time": "2018-05-16T16:58:27.615010Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") ## May use porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:00:04.948356Z",
     "start_time": "2018-05-16T17:00:04.941395Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:01:20.729591Z",
     "start_time": "2018-05-16T17:01:20.688983Z"
    }
   },
   "outputs": [],
   "source": [
    "wrds = [stemmer.stem(wrd) for wrd in _wrds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:02:16.214419Z",
     "start_time": "2018-05-16T17:02:16.207226Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(wrds))\n",
    "print(wrds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:09:28.313287Z",
     "start_time": "2018-05-16T17:09:28.305767Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    txt1 = re.sub('[\\n]',\" \",txt)\n",
    "    txt1 = re.sub('[^A-Za-z ]+', '', txt1)\n",
    "    \n",
    "    _wrds = txt1.split()\n",
    "    stemmer = SnowballStemmer(\"english\") ## May use porter stemmer\n",
    "    wrds = [stemmer.stem(wrd) for wrd in _wrds]\n",
    "    return wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:06:11.852676Z",
     "start_time": "2018-05-16T17:06:11.844542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.TEXT.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:22:05.268035Z",
     "start_time": "2018-05-16T17:14:19.632301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 records processed\n"
     ]
    }
   ],
   "source": [
    "data_set = []\n",
    "vocabulary = []\n",
    "_vocab = defaultdict(int)\n",
    "for i in range(0,df_text.TEXT.size):\n",
    "    txt = process(df_text.TEXT[i])\n",
    "    data_set.append(txt)\n",
    "    \n",
    "    for wrd in txt:\n",
    "        _vocab[wrd] += 1\n",
    "        \n",
    "    vocabulary = vocabulary + txt\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        print(\"%5d records processed\"%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:28:38.157102Z",
     "start_time": "2018-05-16T17:28:38.149204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5069\n"
     ]
    }
   ],
   "source": [
    "print(len(_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vocab = sorted(_vocab.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = _vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:34:21.123993Z",
     "start_time": "2018-05-16T17:34:21.076894Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_map = {}\n",
    "for i in range(0,len(vocab)):\n",
    "    vocab_map[vocab[i][0]] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_map = {}\n",
    "for i in range(0,len(vocabulary)):\n",
    "    vocabulary_map[vocabulary[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:39:53.329776Z",
     "start_time": "2018-05-16T17:39:53.326293Z"
    }
   },
   "outputs": [],
   "source": [
    "a = \"hello my mutenot my only mutenot\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:42:40.383503Z",
     "start_time": "2018-05-16T17:42:40.374700Z"
    }
   },
   "outputs": [],
   "source": [
    "'mutenot' in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:42:11.062088Z",
     "start_time": "2018-05-16T17:42:11.047577Z"
    }
   },
   "outputs": [],
   "source": [
    "a.index('mutenot',6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:52:21.003120Z",
     "start_time": "2018-05-16T17:52:20.996458Z"
    }
   },
   "outputs": [],
   "source": [
    "a = {}\n",
    "1 in a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T18:07:22.918655Z",
     "start_time": "2018-05-16T17:56:09.167579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 records processes\n"
     ]
    }
   ],
   "source": [
    "_train = {}\n",
    "count = 0\n",
    "for feat in data_set:\n",
    "    _feat = [0]*len(vocab)\n",
    "    for i in range(2,len(feat)-2):\n",
    "        \n",
    "        key = vocabulary_map[feat[i]]\n",
    "        if key in _train.keys():\n",
    "            _feat = _train[key]\n",
    "        else:\n",
    "            _feat = [0]*len(vocab)\n",
    "        \n",
    "        try:\n",
    "            _feat[vocab_map[feat[i-2]]] += 1\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            _feat[vocab_map[feat[i-1]]] += 1\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            _feat[vocab_map[feat[i+1]]] += 1\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            _feat[vocab_map[feat[i+2]]] += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        _train[key] = _feat\n",
    "    if count%100 == 0:\n",
    "        print(\"%5d records processes\"%(count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 1757)\n",
      "[3, 11, 5, 6, 1, 1, 2, 1, 2, 0, 4, 0, 0, 0, 10, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 3, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 0, 1, 1, 0, 0, 0, 4, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(vocab[0])\n",
    "print(_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5068"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for k in _train.keys():\n",
    "    X_train.append(_train[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_distance(w1,w2):\n",
    "    res = np.dot(w1,w2)/(np.linalg.norm(w1)*np.linalg.norm(w2))\n",
    "    return 1-res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "clf = NearestNeighbors(n_neighbors=6, algorithm='ball_tree', metric=knn_distance).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Similar to  show  are as follows: ['intranas', 'vers', 'black', 'supraclavicular', 'fluoroscop']\n",
      "Words Similar to  exam  are as follows: ['k', 'pulmonari', 'heart', 'direct', 'campus']\n",
      "Words Similar to  due  are as follows: ['identifi', 'slight', 'bronch', 'isosorbid', 'pancrea']\n",
      "Words Similar to  md  are as follows: ['corsetbelt', 'better', 'cx', 'cultur', 'began']\n",
      "Words Similar to  increas  are as follows: ['appear', 'md', 'better', 'cultur', 'began']\n",
      "Words Similar to  servic  are as follows: ['hep', 'suction', 'bls', 'alfa', 'tobramycin']\n",
      "Words Similar to  bilater  are as follows: ['build', 'doe', 'seen', 'hgb', 'howev']\n",
      "Words Similar to  week  are as follows: ['squeez', 'each', 'pe', 'temperatur', 'vision']\n",
      "Words Similar to  prior  are as follows: ['again', 'abdomin', 'stand', 'feed', 'glass']\n",
      "Words Similar to  iv  are as follows: ['trazadon', 'leaflet', 'crohn', 'qid', 'see']\n"
     ]
    }
   ],
   "source": [
    "key_words = list(vocab_map.keys())[-10:]\n",
    "\n",
    "for wrd in key_words:\n",
    "    dist, idxs = clf.kneighbors([X_train[vocab_map[wrd]]])\n",
    "    temp = []\n",
    "    for i in idxs[0]:\n",
    "        temp.append(_vocab[i][0])\n",
    "    print(\"Words Similar to \",wrd,\" are as follows:\",temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_buckets = defaultdict(list)\n",
    "for i in range(0,kmeans.labels_.shape[0]):\n",
    "    cluster_buckets[kmeans.labels_[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "kclusterer = KMeansClusterer(2, distance=knn_distance, repeats=1, avoid_empty_clusters=True)\n",
    "assigned_clusters = kclusterer.cluster(np.array(X_train), assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_buckets = defaultdict(list)\n",
    "for i in range(0,len(assigned_clusters)):\n",
    "    cluster_buckets[assigned_clusters[i]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the cluster words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_buckets_word = defaultdict(list)\n",
    "for val in cluster_buckets:\n",
    "    for idx in cluster_buckets[val]:\n",
    "        cluster_buckets_word[val].append(_vocab[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_buckets_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Techniques : Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
